{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in `1_notmnist.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size*image_size))\n",
    "    tf_train_labels  = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset  = tf.constant(test_dataset)\n",
    "    \n",
    "    # Variables\n",
    "    weights = tf.Variable(tf.truncated_normal([image_size*image_size, num_labels]))\n",
    "    biases  = tf.Variable(tf.zeros([num_labels]))\n",
    "    beta    = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # Training computation\n",
    "    logits  = tf.matmul(tf_train_dataset, weights) + biases\n",
    "    loss    = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)\n",
    "                            + beta*tf.nn.l2_loss(weights))\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    # Predictions for the training, validation and test data\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    test_prediction  = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beta: 0.0001\n",
      "Minibatch loss at step 0: 20.148298\n",
      "Minibatch accuracy: 7.8%\n",
      "Validation accuracy: 8.0%\n",
      "Minibatch loss at step 1000: 1.446566\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 76.7%\n",
      "Minibatch loss at step 2000: 0.979028\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 78.1%\n",
      "Minibatch loss at step 3000: 1.034497\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 79.4%\n",
      "Test accuracy: 86.8% \n",
      "\n",
      "Beta: 0.0006210526315789474\n",
      "Minibatch loss at step 0: 19.747173\n",
      "Minibatch accuracy: 11.7%\n",
      "Validation accuracy: 13.1%\n",
      "Minibatch loss at step 1000: 1.909736\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 77.5%\n",
      "Minibatch loss at step 2000: 1.005980\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 79.5%\n",
      "Minibatch loss at step 3000: 0.877927\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 81.1%\n",
      "Test accuracy: 88.6% \n",
      "\n",
      "Beta: 0.0011421052631578948\n",
      "Minibatch loss at step 0: 22.443321\n",
      "Minibatch accuracy: 5.5%\n",
      "Validation accuracy: 11.8%\n",
      "Minibatch loss at step 1000: 1.671203\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 79.0%\n",
      "Minibatch loss at step 2000: 0.777960\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 81.1%\n",
      "Minibatch loss at step 3000: 0.747422\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 82.0%\n",
      "Test accuracy: 89.0% \n",
      "\n",
      "Beta: 0.0016631578947368423\n",
      "Minibatch loss at step 0: 23.789623\n",
      "Minibatch accuracy: 7.8%\n",
      "Validation accuracy: 12.3%\n",
      "Minibatch loss at step 1000: 1.499692\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 79.5%\n",
      "Minibatch loss at step 2000: 0.669916\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 81.4%\n",
      "Minibatch loss at step 3000: 0.721751\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 81.9%\n",
      "Test accuracy: 89.2% \n",
      "\n",
      "Beta: 0.0021842105263157894\n",
      "Minibatch loss at step 0: 23.218283\n",
      "Minibatch accuracy: 9.4%\n",
      "Validation accuracy: 10.7%\n",
      "Minibatch loss at step 1000: 1.321610\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 80.4%\n",
      "Minibatch loss at step 2000: 0.612923\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 81.7%\n",
      "Minibatch loss at step 3000: 0.709787\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 81.9%\n",
      "Test accuracy: 89.1% \n",
      "\n",
      "Beta: 0.0027052631578947366\n",
      "Minibatch loss at step 0: 29.343563\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 9.2%\n",
      "Minibatch loss at step 1000: 1.201386\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 80.8%\n",
      "Minibatch loss at step 2000: 0.603985\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 81.7%\n",
      "Minibatch loss at step 3000: 0.714243\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 81.8%\n",
      "Test accuracy: 89.1% \n",
      "\n",
      "Beta: 0.0032263157894736843\n",
      "Minibatch loss at step 0: 27.787270\n",
      "Minibatch accuracy: 11.7%\n",
      "Validation accuracy: 13.0%\n",
      "Minibatch loss at step 1000: 1.020138\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 81.2%\n",
      "Minibatch loss at step 2000: 0.600096\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 81.7%\n",
      "Minibatch loss at step 3000: 0.721276\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 81.8%\n",
      "Test accuracy: 89.1% \n",
      "\n",
      "Beta: 0.0037473684210526316\n",
      "Minibatch loss at step 0: 26.872934\n",
      "Minibatch accuracy: 13.3%\n",
      "Validation accuracy: 17.5%\n",
      "Minibatch loss at step 1000: 0.896547\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.5%\n",
      "Minibatch loss at step 2000: 0.595046\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 81.7%\n",
      "Minibatch loss at step 3000: 0.727582\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 81.7%\n",
      "Test accuracy: 89.0% \n",
      "\n",
      "Beta: 0.004268421052631579\n",
      "Minibatch loss at step 0: 28.726498\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 12.2%\n",
      "Minibatch loss at step 1000: 0.885734\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.3%\n",
      "Minibatch loss at step 2000: 0.601421\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 81.6%\n",
      "Minibatch loss at step 3000: 0.733684\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 81.7%\n",
      "Test accuracy: 89.0% \n",
      "\n",
      "Beta: 0.004789473684210527\n",
      "Minibatch loss at step 0: 30.639162\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 8.6%\n",
      "Minibatch loss at step 1000: 0.849087\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 81.6%\n",
      "Minibatch loss at step 2000: 0.604831\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 81.6%\n",
      "Minibatch loss at step 3000: 0.739360\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 81.6%\n",
      "Test accuracy: 89.0% \n",
      "\n",
      "Beta: 0.005310526315789474\n",
      "Minibatch loss at step 0: 32.988857\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 9.6%\n",
      "Minibatch loss at step 1000: 0.819425\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.5%\n",
      "Minibatch loss at step 2000: 0.609648\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 81.5%\n",
      "Minibatch loss at step 3000: 0.744521\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 81.6%\n",
      "Test accuracy: 88.9% \n",
      "\n",
      "Beta: 0.005831578947368421\n",
      "Minibatch loss at step 0: 32.106590\n",
      "Minibatch accuracy: 11.7%\n",
      "Validation accuracy: 14.4%\n",
      "Minibatch loss at step 1000: 0.793831\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 81.8%\n",
      "Minibatch loss at step 2000: 0.614270\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 81.5%\n",
      "Minibatch loss at step 3000: 0.749695\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 81.6%\n",
      "Test accuracy: 88.9% \n",
      "\n",
      "Beta: 0.006352631578947369\n",
      "Minibatch loss at step 0: 36.277889\n",
      "Minibatch accuracy: 9.4%\n",
      "Validation accuracy: 12.0%\n",
      "Minibatch loss at step 1000: 0.791124\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 81.7%\n",
      "Minibatch loss at step 2000: 0.618864\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 81.5%\n",
      "Minibatch loss at step 3000: 0.754580\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 81.5%\n",
      "Test accuracy: 88.8% \n",
      "\n",
      "Beta: 0.0068736842105263166\n",
      "Minibatch loss at step 0: 38.868904\n",
      "Minibatch accuracy: 5.5%\n",
      "Validation accuracy: 7.1%\n",
      "Minibatch loss at step 1000: 0.788026\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 81.5%\n",
      "Minibatch loss at step 2000: 0.623290\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 81.5%\n",
      "Minibatch loss at step 3000: 0.759239\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.5%\n",
      "Test accuracy: 88.8% \n",
      "\n",
      "Beta: 0.007394736842105264\n",
      "Minibatch loss at step 0: 40.866722\n",
      "Minibatch accuracy: 11.7%\n",
      "Validation accuracy: 13.0%\n",
      "Minibatch loss at step 1000: 0.784580\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 81.5%\n",
      "Minibatch loss at step 2000: 0.627815\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 81.5%\n",
      "Minibatch loss at step 3000: 0.763768\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.5%\n",
      "Test accuracy: 88.8% \n",
      "\n",
      "Beta: 0.00791578947368421\n",
      "Minibatch loss at step 0: 42.608040\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 11.0%\n",
      "Minibatch loss at step 1000: 0.788598\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 81.4%\n",
      "Minibatch loss at step 2000: 0.632092\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 81.4%\n",
      "Minibatch loss at step 3000: 0.768136\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 81.5%\n",
      "Test accuracy: 88.7% \n",
      "\n",
      "Beta: 0.008436842105263158\n",
      "Minibatch loss at step 0: 43.270500\n",
      "Minibatch accuracy: 7.8%\n",
      "Validation accuracy: 11.7%\n",
      "Minibatch loss at step 1000: 0.790000\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 81.4%\n",
      "Minibatch loss at step 2000: 0.636358\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 81.5%\n",
      "Minibatch loss at step 3000: 0.772376\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 81.5%\n",
      "Test accuracy: 88.6% \n",
      "\n",
      "Beta: 0.008957894736842106\n",
      "Minibatch loss at step 0: 42.742683\n",
      "Minibatch accuracy: 14.8%\n",
      "Validation accuracy: 15.0%\n",
      "Minibatch loss at step 1000: 0.793766\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 81.3%\n",
      "Minibatch loss at step 2000: 0.640520\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 81.4%\n",
      "Minibatch loss at step 3000: 0.776496\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 81.5%\n",
      "Test accuracy: 88.6% \n",
      "\n",
      "Beta: 0.009478947368421052\n",
      "Minibatch loss at step 0: 46.770123\n",
      "Minibatch accuracy: 7.8%\n",
      "Validation accuracy: 15.6%\n",
      "Minibatch loss at step 1000: 0.797887\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 81.3%\n",
      "Minibatch loss at step 2000: 0.644426\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 81.3%\n",
      "Minibatch loss at step 3000: 0.780490\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 81.4%\n",
      "Test accuracy: 88.6% \n",
      "\n",
      "Beta: 0.01\n",
      "Minibatch loss at step 0: 49.306454\n",
      "Minibatch accuracy: 11.7%\n",
      "Validation accuracy: 9.5%\n",
      "Minibatch loss at step 1000: 0.800104\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 81.3%\n",
      "Minibatch loss at step 2000: 0.648412\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 81.2%\n",
      "Minibatch loss at step 3000: 0.784389\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 81.4%\n",
      "Test accuracy: 88.6% \n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "beta_vals = np.linspace(1e-4, 1e-2, 20)\n",
    "\n",
    "accuracies = []\n",
    "for beta_ in beta_vals:\n",
    "    print('Beta: {0}'.format(beta_))\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "\n",
    "        for step in range(num_steps):\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "\n",
    "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta : beta_}\n",
    "            _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "            if step % 1000 == 0:\n",
    "                print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "                print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "                print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "        accuracy_score = accuracy(test_prediction.eval(), test_labels)\n",
    "        accuracies.append(accuracy_score)\n",
    "        print(\"Test accuracy: %.1f%% \\n\" % accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0xc5c7f28>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA6wAAAGVCAYAAAD60XwIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npP8Hssl0jr4+fpAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xc53fc18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(16,6))\n",
    "plt.plot(accuracies)\n",
    "plt.xticks(range(beta_vals.shape[0]), ['{0:.5f}'.format(beta) for beta in beta_vals], rotation = 270)\n",
    "plt.title('Influencia de Beta (regularizacao) na acuracia do banco de teste')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape = (batch_size, image_size*image_size))\n",
    "    tf_train_labels  = tf.placeholder(tf.float32, shape = (batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset  = tf.constant(test_dataset)\n",
    "    \n",
    "    # Variables\n",
    "    weights = tf.Variable(tf.truncated_normal([image_size*image_size, hidden_nodes]))\n",
    "    biases  = tf.Variable(tf.zeros([hidden_nodes]))\n",
    "    hidden_weights = tf.Variable(tf.truncated_normal([hidden_nodes, num_labels]))\n",
    "    hidden_biases  = tf.Variable(tf.zeros([num_labels]))\n",
    "    beta    = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # Training computation\n",
    "    relu = tf.nn.relu(tf.matmul(tf_train_dataset, weights) + biases)\n",
    "    \n",
    "    logits = tf.matmul(relu, hidden_weights) + hidden_biases\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels)\n",
    "                          + beta*tf.nn.l2_loss(weights))\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    # Predictions\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    valid_prediction_hidden = tf.nn.relu(tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(valid_prediction_hidden, hidden_weights) + hidden_biases)\n",
    "    \n",
    "    test_prediction_hidden = tf.nn.relu(tf.matmul(tf_test_dataset, weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(test_prediction_hidden, hidden_weights) + hidden_biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beta: 0.0001\n",
      "Minibatch loss at step 0: 374.14239501953125\n",
      "Minibatch accuracy: 4.7%\n",
      "Validation accuracy: 30.9%\n",
      "Minibatch loss at step 1000: 35.3478889465332\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 80.7%\n",
      "Minibatch loss at step 2000: 26.6298770904541\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 81.5%\n",
      "Minibatch loss at step 3000: 23.67142105102539\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 82.3%\n",
      "Test accuracy: 89.5% \n",
      "\n",
      "Beta: 0.0006210526315789474\n",
      "Minibatch loss at step 0: 564.4607543945312\n",
      "Minibatch accuracy: 7.8%\n",
      "Validation accuracy: 34.4%\n",
      "Minibatch loss at step 1000: 107.47504425048828\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 81.3%\n",
      "Minibatch loss at step 2000: 54.594757080078125\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 83.6%\n",
      "Minibatch loss at step 3000: 29.382781982421875\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 85.0%\n",
      "Test accuracy: 91.8% \n",
      "\n",
      "Beta: 0.0011421052631578948\n",
      "Minibatch loss at step 0: 695.8947143554688\n",
      "Minibatch accuracy: 10.9%\n",
      "Validation accuracy: 38.1%\n",
      "Minibatch loss at step 1000: 115.797607421875\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 81.9%\n",
      "Minibatch loss at step 2000: 35.49512481689453\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 85.2%\n",
      "Minibatch loss at step 3000: 11.659749984741211\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 86.6%\n",
      "Test accuracy: 93.1% \n",
      "\n",
      "Beta: 0.0016631578947368423\n",
      "Minibatch loss at step 0: 822.4090576171875\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 29.7%\n",
      "Minibatch loss at step 1000: 97.87028503417969\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 81.4%\n",
      "Minibatch loss at step 2000: 18.303226470947266\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 85.4%\n",
      "Minibatch loss at step 3000: 3.956815481185913\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 86.6%\n",
      "Test accuracy: 93.1% \n",
      "\n",
      "Beta: 0.0021842105263157894\n",
      "Minibatch loss at step 0: 1032.17333984375\n",
      "Minibatch accuracy: 13.3%\n",
      "Validation accuracy: 23.4%\n",
      "Minibatch loss at step 1000: 76.6001205444336\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 81.1%\n",
      "Minibatch loss at step 2000: 8.76510238647461\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 86.2%\n",
      "Minibatch loss at step 3000: 1.5376667976379395\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 86.3%\n",
      "Test accuracy: 92.8% \n",
      "\n",
      "Beta: 0.0027052631578947366\n",
      "Minibatch loss at step 0: 1194.4990234375\n",
      "Minibatch accuracy: 10.9%\n",
      "Validation accuracy: 34.0%\n",
      "Minibatch loss at step 1000: 56.09264373779297\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 83.0%\n",
      "Minibatch loss at step 2000: 4.058112621307373\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 86.4%\n",
      "Minibatch loss at step 3000: 0.8082207441329956\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 86.3%\n",
      "Test accuracy: 92.7% \n",
      "\n",
      "Beta: 0.0032263157894736843\n",
      "Minibatch loss at step 0: 1348.88623046875\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 22.2%\n",
      "Minibatch loss at step 1000: 39.999847412109375\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 83.3%\n",
      "Minibatch loss at step 2000: 1.9963961839675903\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 85.8%\n",
      "Minibatch loss at step 3000: 0.6577739119529724\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 86.3%\n",
      "Test accuracy: 92.5% \n",
      "\n",
      "Beta: 0.0037473684210526316\n",
      "Minibatch loss at step 0: 1512.327392578125\n",
      "Minibatch accuracy: 14.8%\n",
      "Validation accuracy: 35.0%\n",
      "Minibatch loss at step 1000: 27.714519500732422\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 83.8%\n",
      "Minibatch loss at step 2000: 1.1166975498199463\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 85.7%\n",
      "Minibatch loss at step 3000: 0.611587405204773\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 86.1%\n",
      "Test accuracy: 92.5% \n",
      "\n",
      "Beta: 0.004268421052631579\n",
      "Minibatch loss at step 0: 1636.5113525390625\n",
      "Minibatch accuracy: 10.9%\n",
      "Validation accuracy: 19.9%\n",
      "Minibatch loss at step 1000: 18.947595596313477\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 84.0%\n",
      "Minibatch loss at step 2000: 0.7604336142539978\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 85.5%\n",
      "Minibatch loss at step 3000: 0.629491925239563\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 86.0%\n",
      "Test accuracy: 92.2% \n",
      "\n",
      "Beta: 0.004789473684210527\n",
      "Minibatch loss at step 0: 1842.6932373046875\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 29.1%\n",
      "Minibatch loss at step 1000: 12.937846183776855\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 84.2%\n",
      "Minibatch loss at step 2000: 0.6350855827331543\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 85.0%\n",
      "Minibatch loss at step 3000: 0.6490934491157532\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 85.5%\n",
      "Test accuracy: 92.1% \n",
      "\n",
      "Beta: 0.005310526315789474\n",
      "Minibatch loss at step 0: 2117.975341796875\n",
      "Minibatch accuracy: 9.4%\n",
      "Validation accuracy: 34.8%\n",
      "Minibatch loss at step 1000: 8.758137702941895\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 84.0%\n",
      "Minibatch loss at step 2000: 0.5476298928260803\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 84.9%\n",
      "Minibatch loss at step 3000: 0.6337879300117493\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 85.5%\n",
      "Test accuracy: 91.9% \n",
      "\n",
      "Beta: 0.005831578947368421\n",
      "Minibatch loss at step 0: 2122.928466796875\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 36.6%\n",
      "Minibatch loss at step 1000: 5.956238746643066\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 83.8%\n",
      "Minibatch loss at step 2000: 0.5084431767463684\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 85.2%\n",
      "Minibatch loss at step 3000: 0.6184290051460266\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 85.2%\n",
      "Test accuracy: 91.8% \n",
      "\n",
      "Beta: 0.006352631578947369\n",
      "Minibatch loss at step 0: 2287.4130859375\n",
      "Minibatch accuracy: 7.8%\n",
      "Validation accuracy: 22.8%\n",
      "Minibatch loss at step 1000: 4.117456436157227\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 83.2%\n",
      "Minibatch loss at step 2000: 0.5206314921379089\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 84.8%\n",
      "Minibatch loss at step 3000: 0.6000299453735352\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 85.1%\n",
      "Test accuracy: 91.6% \n",
      "\n",
      "Beta: 0.0068736842105263166\n",
      "Minibatch loss at step 0: 2518.94970703125\n",
      "Minibatch accuracy: 10.9%\n",
      "Validation accuracy: 29.1%\n",
      "Minibatch loss at step 1000: 2.9374241828918457\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 83.5%\n",
      "Minibatch loss at step 2000: 0.5097317099571228\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 84.3%\n",
      "Minibatch loss at step 3000: 0.6883330941200256\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 85.0%\n",
      "Test accuracy: 91.5% \n",
      "\n",
      "Beta: 0.007394736842105264\n",
      "Minibatch loss at step 0: 2648.4658203125\n",
      "Minibatch accuracy: 13.3%\n",
      "Validation accuracy: 32.3%\n",
      "Minibatch loss at step 1000: 2.2540283203125\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 83.3%\n",
      "Minibatch loss at step 2000: 0.516047477722168\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 83.9%\n",
      "Minibatch loss at step 3000: 0.6257885694503784\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 84.7%\n",
      "Test accuracy: 91.0% \n",
      "\n",
      "Beta: 0.00791578947368421\n",
      "Minibatch loss at step 0: 2776.010009765625\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 30.7%\n",
      "Minibatch loss at step 1000: 1.6552448272705078\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 83.4%\n",
      "Minibatch loss at step 2000: 0.5195642709732056\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 84.6%\n",
      "Minibatch loss at step 3000: 0.6370677947998047\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 84.8%\n",
      "Test accuracy: 91.3% \n",
      "\n",
      "Beta: 0.008436842105263158\n",
      "Minibatch loss at step 0: 2933.41943359375\n",
      "Minibatch accuracy: 9.4%\n",
      "Validation accuracy: 27.1%\n",
      "Minibatch loss at step 1000: 1.3061871528625488\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 83.0%\n",
      "Minibatch loss at step 2000: 0.5331288576126099\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 84.2%\n",
      "Minibatch loss at step 3000: 0.722531795501709\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 84.5%\n",
      "Test accuracy: 91.0% \n",
      "\n",
      "Beta: 0.008957894736842106\n",
      "Minibatch loss at step 0: 3193.35205078125\n",
      "Minibatch accuracy: 7.0%\n",
      "Validation accuracy: 31.6%\n",
      "Minibatch loss at step 1000: 1.15908682346344\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.9%\n",
      "Minibatch loss at step 2000: 0.5735572576522827\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 83.5%\n",
      "Minibatch loss at step 3000: 0.6564048528671265\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 84.4%\n",
      "Test accuracy: 91.0% \n",
      "\n",
      "Beta: 0.009478947368421052\n",
      "Minibatch loss at step 0: 3238.5634765625\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 35.6%\n",
      "Minibatch loss at step 1000: 1.0373780727386475\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 80.6%\n",
      "Minibatch loss at step 2000: 0.5617691278457642\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 83.8%\n",
      "Minibatch loss at step 3000: 0.6961638331413269\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 84.5%\n",
      "Test accuracy: 90.9% \n",
      "\n",
      "Beta: 0.01\n",
      "Minibatch loss at step 0: 3490.109375\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 30.4%\n",
      "Minibatch loss at step 1000: 0.9558359384536743\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 82.0%\n",
      "Minibatch loss at step 2000: 0.5309367179870605\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 83.8%\n",
      "Minibatch loss at step 3000: 0.6649544835090637\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 84.7%\n",
      "Test accuracy: 91.3% \n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "beta_vals = np.linspace(1e-4, 1e-2, 20)\n",
    "\n",
    "accuracies = []\n",
    "for beta_ in beta_vals:\n",
    "    print('Beta: {0}'.format(beta_))\n",
    "    \n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "        \n",
    "        for step in range(num_steps):\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "            \n",
    "            feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels, beta:beta_}\n",
    "            _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "            \n",
    "            if step % 1000 == 0:\n",
    "                print(\"Minibatch loss at step {0}: {1}\".format(step, l))\n",
    "                print(\"Minibatch accuracy: {0:.1f}%\".format(accuracy(predictions, batch_labels)))\n",
    "                print(\"Validation accuracy: {0:.1f}%\".format(accuracy(valid_prediction.eval(), valid_labels)))\n",
    "                \n",
    "        accuracy_score = accuracy(test_prediction.eval(), test_labels)\n",
    "        accuracies.append(accuracy_score)\n",
    "        print(\"Test accuracy: {0:.1f}% \\n\".format(accuracy_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0xc5b2978>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA6wAAAGVCAYAAAD60XwIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xc5aceb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(16,6))\n",
    "plt.plot(accuracies)\n",
    "plt.xticks(range(beta_vals.shape[0]), ['{0:.5f}'.format(beta) for beta in beta_vals], rotation = 270)\n",
    "plt.title('Influencia de Beta (regularizacao) na acuracia do banco de teste')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 50000\n",
    "hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape = (batch_size, image_size*image_size))\n",
    "    tf_train_labels  = tf.placeholder(tf.float32, shape = (batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset  = tf.constant(test_dataset)\n",
    "    \n",
    "    # Variables\n",
    "    weights = tf.Variable(tf.truncated_normal([image_size*image_size, hidden_nodes]))\n",
    "    biases  = tf.Variable(tf.zeros([hidden_nodes]))\n",
    "    hidden_weights = tf.Variable(tf.truncated_normal([hidden_nodes, num_labels]))\n",
    "    hidden_biases  = tf.Variable(tf.zeros([num_labels]))\n",
    "    beta = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # Training computation\n",
    "    relu = tf.nn.relu(tf.matmul(tf_train_dataset, weights) + biases)\n",
    "    \n",
    "    logits = tf.matmul(relu, hidden_weights) + hidden_biases\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)\n",
    "                         + beta*tf.nn.l2_loss(weights))\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    # Predictions\n",
    "    train_predictions = tf.nn.softmax(logits)\n",
    "    \n",
    "    valid_predictions_hidden = tf.nn.relu(tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    valid_predictions = tf.nn.softmax(tf.matmul(valid_predictions_hidden, hidden_weights) + hidden_biases)\n",
    "    \n",
    "    test_predictions_hidden = tf.nn.relu(tf.matmul(tf_test_dataset, weights) + biases)\n",
    "    test_predictions = tf.nn.softmax(tf.matmul(test_predictions_hidden, hidden_weights) + hidden_biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 0: 1011.6\n",
      "Minibatch accuracy: 12.7%\n",
      "Validation accuracy: 37.4%\n",
      "Minibatch loss at step 5: 810.0\n",
      "Minibatch accuracy: 63.8%\n",
      "Validation accuracy: 69.2%\n",
      "Minibatch loss at step 10: 735.6\n",
      "Minibatch accuracy: 76.7%\n",
      "Validation accuracy: 77.6%\n",
      "Minibatch loss at step 15: 693.8\n",
      "Minibatch accuracy: 80.6%\n",
      "Validation accuracy: 79.5%\n",
      "Minibatch loss at step 20: 677.9\n",
      "Minibatch accuracy: 80.9%\n",
      "Validation accuracy: 79.8%\n",
      "Minibatch loss at step 25: 665.7\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 79.7%\n",
      "Minibatch loss at step 30: 680.9\n",
      "Minibatch accuracy: 75.4%\n",
      "Validation accuracy: 78.6%\n",
      "Minibatch loss at step 35: 648.6\n",
      "Minibatch accuracy: 81.3%\n",
      "Validation accuracy: 79.8%\n",
      "Minibatch loss at step 40: 639.2\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 79.4%\n",
      "Minibatch loss at step 45: 630.9\n",
      "Minibatch accuracy: 80.9%\n",
      "Validation accuracy: 78.5%\n",
      "Minibatch loss at step 50: 687.4\n",
      "Minibatch accuracy: 74.3%\n",
      "Validation accuracy: 75.0%\n",
      "Test accuracy: 82.3\n"
     ]
    }
   ],
   "source": [
    "num_steps = 51\n",
    "num_batches = 3\n",
    "beta_ = 0.00218\n",
    "    \n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        #offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        offset = step % num_batches\n",
    "\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), : ]\n",
    "\n",
    "        feed_dict = {tf_train_dataset:batch_data, tf_train_labels:batch_labels, beta:beta_}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_predictions], feed_dict=feed_dict)\n",
    "\n",
    "        if step % 5 == 0:\n",
    "            print(\"Minibatch loss at step {0}: {1:.1f}\".format(step, l))\n",
    "            print(\"Minibatch accuracy: {0:.1f}%\".format(accuracy(predictions, batch_labels)))\n",
    "            print(\"Validation accuracy: {0:.1f}%\".format(accuracy(valid_predictions.eval(), valid_labels)))\n",
    "\n",
    "    print(\"Test accuracy: {0:.1f}\".format(accuracy(test_predictions.eval(), test_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Como o nmero de batches  pequeno (=3), h tambm muitos parmetros para otimizar, mesmo com a regularizao, a capacidade de generalizao  muito fraca. Logo, com apenas poucos passos, a rede atinge 100% no treinamento, mas no consegue boas taxas nos bancos de validao e teste.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size*image_size))\n",
    "    tf_train_labels  = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset  = tf.constant(test_dataset)\n",
    "    \n",
    "    # Variables\n",
    "    weights = tf.Variable(tf.truncated_normal([image_size*image_size, hidden_nodes]))\n",
    "    biases  = tf.Variable(tf.zeros([hidden_nodes]))\n",
    "    hidden_weights = tf.Variable(tf.truncated_normal([hidden_nodes, num_labels]))\n",
    "    hidden_biases  = tf.Variable(tf.zeros([num_labels]))\n",
    "    beta = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # Training computation\n",
    "    relu = tf.nn.relu(tf.matmul(tf_train_dataset, weights) + biases)\n",
    "    dropout = tf.nn.dropout(relu, 0.5)\n",
    "    \n",
    "    logits = tf.matmul(dropout, hidden_weights) + hidden_biases\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels)\n",
    "                         + beta*tf.nn.l2_loss(weights))\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    # Predictions\n",
    "    train_predictions = tf.nn.softmax(logits)\n",
    "    \n",
    "    valid_predictions_hidden = tf.nn.relu(tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    valid_predictions = tf.nn.softmax(tf.matmul(valid_predictions_hidden, hidden_weights) + hidden_biases)\n",
    "    \n",
    "    test_predictions_hidden = tf.nn.relu(tf.matmul(tf_test_dataset, weights) + biases)\n",
    "    test_predictions = tf.nn.softmax(tf.matmul(test_predictions_hidden, hidden_weights) + hidden_biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 0: 1118.2\n",
      "Minibatch accuracy: 9.0%\n",
      "Validation accuracy: 38.6%\n",
      "Minibatch loss at step 5: 962.0\n",
      "Minibatch accuracy: 63.3%\n",
      "Validation accuracy: 66.4%\n",
      "Minibatch loss at step 10: 679.8\n",
      "Minibatch accuracy: 92.6%\n",
      "Validation accuracy: 73.7%\n",
      "Minibatch loss at step 15: 658.4\n",
      "Minibatch accuracy: 95.7%\n",
      "Validation accuracy: 74.3%\n",
      "Minibatch loss at step 20: 651.5\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 74.2%\n",
      "Minibatch loss at step 25: 643.8\n",
      "Minibatch accuracy: 97.3%\n",
      "Validation accuracy: 75.4%\n",
      "Minibatch loss at step 30: 640.9\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 75.2%\n",
      "Minibatch loss at step 35: 631.6\n",
      "Minibatch accuracy: 98.8%\n",
      "Validation accuracy: 75.6%\n",
      "Minibatch loss at step 40: 620.7\n",
      "Minibatch accuracy: 98.8%\n",
      "Validation accuracy: 75.1%\n",
      "Minibatch loss at step 45: 615.3\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 74.9%\n",
      "Minibatch loss at step 50: 607.2\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 73.0%\n",
      "Minibatch loss at step 55: 600.3\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 75.2%\n",
      "Minibatch loss at step 60: 594.7\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 74.8%\n",
      "Minibatch loss at step 65: 586.8\n",
      "Minibatch accuracy: 99.6%\n",
      "Validation accuracy: 75.2%\n",
      "Minibatch loss at step 70: 581.0\n",
      "Minibatch accuracy: 98.8%\n",
      "Validation accuracy: 75.8%\n",
      "Minibatch loss at step 75: 574.5\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 74.6%\n",
      "Minibatch loss at step 80: 569.2\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 75.5%\n",
      "Minibatch loss at step 85: 561.5\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 75.6%\n",
      "Minibatch loss at step 90: 556.6\n",
      "Minibatch accuracy: 98.8%\n",
      "Validation accuracy: 75.6%\n",
      "Minibatch loss at step 95: 549.6\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 75.4%\n",
      "Minibatch loss at step 100: 544.2\n",
      "Minibatch accuracy: 98.8%\n",
      "Validation accuracy: 75.1%\n",
      "Test accuracy: 82.1%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 101\n",
    "num_batches = 3\n",
    "beta_ = 0.00218\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        offset  = step % num_batches\n",
    "        \n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        \n",
    "        feed_dict = {tf_train_dataset:batch_data, tf_train_labels:batch_labels, beta:beta_}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_predictions], feed_dict=feed_dict)\n",
    "        \n",
    "        if step % 5 == 0:\n",
    "            print(\"Minibatch loss at step {0}: {1:.1f}\".format(step, l))\n",
    "            print(\"Minibatch accuracy: {0:.1f}%\".format(accuracy(predictions, batch_labels)))\n",
    "            print(\"Validation accuracy: {0:.1f}%\".format(accuracy(valid_predictions.eval(), valid_labels)))\n",
    "\n",
    "    print(\"Test accuracy: {0:.1f}%\".format(accuracy(test_predictions.eval(), test_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**\n",
    "Pode-se perceber que a acurcia de 100% nos minibatchs  mais difcil de conseguir ou, pelo menos, manter. A acurcia final no banco de teste foi melhorada em 3,1%.\n",
    "**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "hidden_nodes_1 = 1024\n",
    "hidden_nodes_2 = 100\n",
    "beta = 0.00218\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # input\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape = (batch_size, image_size*image_size))\n",
    "    tf_train_labels  = tf.placeholder(tf.float32, shape = (batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset  = tf.constant(test_dataset)\n",
    "    \n",
    "    # Variables\n",
    "    weights_1 = tf.Variable(tf.truncated_normal([image_size*image_size, hidden_nodes_1], stddev=np.sqrt(2.0 / (image_size*image_size))))\n",
    "    weights_2 = tf.Variable(tf.truncated_normal([hidden_nodes_1, hidden_nodes_2], stddev=np.sqrt(2.0 / hidden_nodes_1)))\n",
    "    weights_3 = tf.Variable(tf.truncated_normal([hidden_nodes_2, num_labels], stddev=np.sqrt(2.0 / hidden_nodes_2)))\n",
    "    biases_1  = tf.Variable(tf.zeros([hidden_nodes_1]))\n",
    "    biases_2  = tf.Variable(tf.zeros([hidden_nodes_2]))\n",
    "    biases_3  = tf.Variable(tf.zeros([num_labels]))\n",
    "    global_step = tf.Variable(0)\n",
    "    \n",
    "    # Training computation\n",
    "    hidden_relu_1 = tf.nn.relu(tf.matmul(tf_train_dataset, weights_1) + biases_1)    \n",
    "    hidden_relu_2 = tf.nn.relu(tf.matmul(hidden_relu_1, weights_2) + biases_2)\n",
    "    \n",
    "    logits = tf.matmul(hidden_relu_2, weights_3) + biases_3\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels)\n",
    "                         + beta*(tf.nn.l2_loss(weights_1) + tf.nn.l2_loss(weights_2) + tf.nn.l2_loss(weights_3)))\n",
    "    \n",
    "    # Optimizer\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, 1000, 0.5, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    \n",
    "    # Predictions\n",
    "    train_predictions = tf.nn.softmax(logits)\n",
    "    \n",
    "    valid_predictions_1 = tf.nn.relu(tf.matmul(tf_valid_dataset, weights_1) + biases_1)\n",
    "    valid_predictions_2 = tf.nn.relu(tf.matmul(valid_predictions_1, weights_2) + biases_2)\n",
    "    valid_predictions   = tf.nn.softmax(tf.matmul(valid_predictions_2, weights_3) + biases_3)\n",
    "    \n",
    "    test_predictions_1 = tf.nn.relu(tf.matmul(tf_test_dataset, weights_1) + biases_1)\n",
    "    test_predictions_2 = tf.nn.relu(tf.matmul(test_predictions_1, weights_2) + biases_2)\n",
    "    test_predictions   = tf.nn.softmax(tf.matmul(test_predictions_2, weights_3) + biases_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 0: 4.3\n",
      "Minibatch accuracy: 7.0%\n",
      "Validation accuracy: 29.6%\n",
      "Minibatch loss at step 500: 1.1\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 85.2%\n",
      "Minibatch loss at step 1000: 0.8\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 86.0%\n",
      "Minibatch loss at step 1500: 0.5\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 87.3%\n",
      "Minibatch loss at step 2000: 0.5\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 87.3%\n",
      "Minibatch loss at step 2500: 0.5\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 87.8%\n",
      "Minibatch loss at step 3000: 0.5\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.8%\n",
      "Minibatch loss at step 3500: 0.6\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 88.3%\n",
      "Minibatch loss at step 4000: 0.5\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 4500: 0.5\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 5000: 0.5\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 88.7%\n",
      "Test accuracy: 94.6\n"
     ]
    }
   ],
   "source": [
    "num_steps = 5001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        \n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        \n",
    "        feed_dict = {tf_train_dataset:batch_data, tf_train_labels:batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_predictions], feed_dict=feed_dict)\n",
    "        \n",
    "        if step % 500 == 0:\n",
    "            print(\"Minibatch loss at step {0}: {1:.1f}\".format(step, l))\n",
    "            print(\"Minibatch accuracy: {0:.1f}%\".format(accuracy(predictions, batch_labels)))\n",
    "            print(\"Validation accuracy: {0:.1f}%\".format(accuracy(valid_predictions.eval(), valid_labels)))\n",
    "\n",
    "    print(\"Test accuracy: {0:.1f}\".format(accuracy(test_predictions.eval(), test_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "hidden_nodes_1 = 1024\n",
    "hidden_nodes_2 = 256\n",
    "hidden_nodes_3 = 128\n",
    "beta = 1e-3\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # input\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape = (batch_size, image_size*image_size))\n",
    "    tf_train_labels  = tf.placeholder(tf.float32, shape = (batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset  = tf.constant(test_dataset)\n",
    "    \n",
    "    # Variables\n",
    "    weights_1 = tf.Variable(tf.truncated_normal([image_size*image_size, hidden_nodes_1], stddev=np.sqrt(2.0 / (image_size*image_size))))\n",
    "    weights_2 = tf.Variable(tf.truncated_normal([hidden_nodes_1, hidden_nodes_2], stddev=np.sqrt(2.0 / hidden_nodes_1)))\n",
    "    weights_3 = tf.Variable(tf.truncated_normal([hidden_nodes_2, hidden_nodes_3], stddev=np.sqrt(2.0 / hidden_nodes_2)))\n",
    "    weights_4 = tf.Variable(tf.truncated_normal([hidden_nodes_3, num_labels], stddev=np.sqrt(2.0 / hidden_nodes_3)))\n",
    "    biases_1  = tf.Variable(tf.zeros([hidden_nodes_1]))\n",
    "    biases_2  = tf.Variable(tf.zeros([hidden_nodes_2]))\n",
    "    biases_3  = tf.Variable(tf.zeros([hidden_nodes_3]))\n",
    "    biases_4  = tf.Variable(tf.zeros([num_labels]))\n",
    "    global_step = tf.Variable(0)\n",
    "    \n",
    "    # Training computation\n",
    "    hidden_relu_1 = tf.nn.relu(tf.matmul(tf_train_dataset, weights_1) + biases_1)\n",
    "    dropout_1 = tf.nn.dropout(hidden_relu_1, 0.5)\n",
    "    \n",
    "    hidden_relu_2 = tf.nn.relu(tf.matmul(dropout_1, weights_2) + biases_2)    \n",
    "    hidden_relu_3 = tf.nn.relu(tf.matmul(hidden_relu_2, weights_3) + biases_3)\n",
    "    \n",
    "    logits = tf.matmul(hidden_relu_3, weights_4) + biases_4\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels)\n",
    "                         + beta*(tf.nn.l2_loss(weights_1) + tf.nn.l2_loss(weights_2) + \n",
    "                                 tf.nn.l2_loss(weights_3) + tf.nn.l2_loss(weights_4)))\n",
    "    \n",
    "    # Optimizer\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, 4000, 0.5, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    \n",
    "    # Predictions\n",
    "    train_predictions = tf.nn.softmax(logits)\n",
    "    \n",
    "    valid_predictions_1 = tf.nn.relu(tf.matmul(tf_valid_dataset, weights_1) + biases_1)\n",
    "    valid_predictions_2 = tf.nn.relu(tf.matmul(valid_predictions_1, weights_2) + biases_2)\n",
    "    valid_predictions_3 = tf.nn.relu(tf.matmul(valid_predictions_2, weights_3) + biases_3)\n",
    "    valid_predictions   = tf.nn.softmax(tf.matmul(valid_predictions_3, weights_4) + biases_4)\n",
    "    \n",
    "    test_predictions_1 = tf.nn.relu(tf.matmul(tf_test_dataset, weights_1) + biases_1)\n",
    "    test_predictions_2 = tf.nn.relu(tf.matmul(test_predictions_1, weights_2) + biases_2)\n",
    "    test_predictions_3 = tf.nn.relu(tf.matmul(test_predictions_2, weights_3) + biases_3)\n",
    "    test_predictions   = tf.nn.softmax(tf.matmul(test_predictions_3, weights_4) + biases_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 0: 3.47364\n",
      "Minibatch accuracy: 9.4%\n",
      "Validation accuracy: 25.8%\n",
      "Minibatch loss at step 1000: 0.84605\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at step 2000: 0.64179\n",
      "Minibatch accuracy: 87.1%\n",
      "Validation accuracy: 86.9%\n",
      "Minibatch loss at step 3000: 0.60596\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 88.0%\n",
      "Minibatch loss at step 4000: 0.49939\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 5000: 0.53701\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 6000: 0.42958\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 7000: 0.40041\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 8000: 0.46084\n",
      "Minibatch accuracy: 89.5%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 9000: 0.43164\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.0%\n",
      "Test accuracy: 95.7%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 10000\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        \n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        \n",
    "        feed_dict = {tf_train_dataset:batch_data, tf_train_labels:batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_predictions], feed_dict=feed_dict)\n",
    "        \n",
    "        if step % 1000 == 0:\n",
    "            print(\"Minibatch loss at step {0}: {1:.5f}\".format(step, l))\n",
    "            print(\"Minibatch accuracy: {0:.1f}%\".format(accuracy(predictions, batch_labels)))\n",
    "            print(\"Validation accuracy: {0:.1f}%\".format(accuracy(valid_predictions.eval(), valid_labels)))\n",
    "\n",
    "    print(\"Test accuracy: {0:.1f}%\".format(accuracy(test_predictions.eval(), test_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esse foi o meu melhor resultado: **95.7%**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
